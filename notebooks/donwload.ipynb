{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf728f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy as ob\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import signal\n",
    "from matplotlib import colormaps\n",
    "\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "\n",
    "from obspy.core.inventory import read_inventory\n",
    "from obspy.core.inventory import read_inventory\n",
    "from obspy.core import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8a4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(trace,interpolation_limit=1):\n",
    "    \"\"\"Snippet to interpolate missing data.  \n",
    "\n",
    "    The SHZ traces have missing data samples 3-4 times every 32 samples. \n",
    "    Providing the seed data with these missing data would mean using very \n",
    "    large files. Instead, we provide the data with -1 replacing the gaps. \n",
    "    To change the files to interpolate across the gaps, use this simple method to \n",
    "    replace the -1 values. The trace is modified, and a mask is applied at \n",
    "    the end if necessary. \n",
    "\n",
    "    :type stream: :class:`~obspy.core.Trace` \n",
    "    :param trace: A data trace\n",
    "    :type interpolation_limit: int \n",
    "    :param interpolation_limit: Limit for interpolation. Defaults to 1. For\n",
    "      more information read the options for the `~pandas.Series.interpolate`\n",
    "      method. \n",
    "\n",
    "    :return: original_mask :class:`~numpy.ndarray` or class:`~numpy.bool_`\n",
    "       Returns the original mask, before any interpolation is made. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    trace.data = np.ma.masked_where(trace.data == -1, trace.data)\n",
    "    original_mask = np.ma.getmask(trace.data)\n",
    "    data_series = pd.Series(trace.data)\n",
    "    # data_series.replace(-1.0, pd.NA, inplace=True)\n",
    "    data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n",
    "    data_series.fillna(-1.0, inplace=True)\n",
    "    trace.data=data_series.to_numpy(dtype=int) \n",
    "    trace.data = np.ma.masked_where(trace.data == -1, trace.data)\n",
    "    return original_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d345c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Apollo(starttime=None, endtime=None, station=\"A11\", channel=\"SHZ\", \n",
    "                units=\"DU\", year=None, month=None):\n",
    "    \"\"\"\n",
    "    Download seismogram stream for given parameters.\n",
    "    \n",
    "    If units=='DU' (default), returns raw digital units.\n",
    "    If units=='ACC', instrument response is removed so that output is acceleration.\n",
    "    \n",
    "    You can specify either:\n",
    "      1) starttime and endtime (as UTCDateTime objects), OR\n",
    "      2) year and month (as integers) to retrieve the entire month.\n",
    "\n",
    "    :param starttime: ObsPy UTCDateTime start\n",
    "    :param endtime:   ObsPy UTCDateTime end\n",
    "    :param station:   Station code (e.g., 'A11')\n",
    "    :param channel:   Channel code (e.g., 'SHZ')\n",
    "    :param units:     'DU' for raw digital units, or 'ACC' for acceleration\n",
    "    :param year:      Optional integer specifying year if selecting entire month\n",
    "    :param month:     Optional integer specifying month if selecting entire month\n",
    "    :return:          Tuple of:\n",
    "                      (Trace with mask applied, [starttime, endtime, station, channel])\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from obspy import UTCDateTime\n",
    "    from obspy.clients.fdsn import Client\n",
    "    \n",
    "    # --- 1. Figure out time window ---\n",
    "    if (starttime is not None or endtime is not None) and (year is not None or month is not None):\n",
    "        raise ValueError(\"Please specify either (starttime, endtime) OR (year, month), not both.\")\n",
    "    \n",
    "    if year is not None and month is not None:\n",
    "        # Create a starttime as the first day of the given month\n",
    "        starttime = UTCDateTime(year, month, 1, 0, 0, 0)\n",
    "        # Create an endtime as the last second of that same month\n",
    "        if month == 12:\n",
    "            endtime = UTCDateTime(year + 1, 1, 1, 0, 0, 0) - 1\n",
    "        else:\n",
    "            endtime = UTCDateTime(year, month + 1, 1, 0, 0, 0) - 1\n",
    "    else:\n",
    "        # If no year/month was given, we assume starttime/endtime are valid\n",
    "        if starttime is None or endtime is None:\n",
    "            raise ValueError(\"You must provide either (starttime, endtime) OR (year, month).\")\n",
    "\n",
    "\n",
    "    # --- 2. Use FDSN client to get data and process ---\n",
    "    client = Client(\"IRIS\")\n",
    "\n",
    "    # Retrieve station metadata (inventory) for response\n",
    "    inv = client.get_stations(starttime=starttime, endtime=endtime,\n",
    "                              network='XA', sta=station, loc='*', channel=channel,\n",
    "                              level=\"response\")\n",
    "    \n",
    "    # Retrieve waveforms\n",
    "    st = client.get_waveforms(network='XA', station=station, channel=channel, location='*',\n",
    "                              starttime=starttime, endtime=endtime)\n",
    "    st.merge()\n",
    "    if not st:\n",
    "             print(f\"Warning: No data returned via FDSN for {station} {channel} between {starttime} and {endtime}\")\n",
    "             return None, [starttime, endtime, station, channel]\n",
    "    for tr in st:\n",
    "        if tr.data is not None and len(tr.data) > 0 and np.issubdtype(tr.data.dtype, np.number):\n",
    "            try:\n",
    "                # Calculate the median value of the trace data\n",
    "                # Use np.ma.median if data might already contain masks/NaNs after merging\n",
    "                if isinstance(tr.data, np.ma.MaskedArray):\n",
    "                    # Calculate median ignoring masked values\n",
    "                    median_val = np.ma.median(tr.data)\n",
    "                else:\n",
    "                    # Calculate median on the numpy array\n",
    "                    median_val = np.median(tr.data)\n",
    "\n",
    "                # Check if median calculation was successful (e.g., not NaN)\n",
    "                if median_val is not None and not np.isnan(median_val):\n",
    "                    invalid_value = median_val - 511\n",
    "\n",
    "                    # Create a boolean mask where data equals the invalid value\n",
    "                    invalid_mask = (tr.data == invalid_value)\n",
    "\n",
    "                    # Apply the mask if any invalid values were found\n",
    "                    if np.any(invalid_mask):\n",
    "                        # Ensure the data is a masked array before applying the mask\n",
    "                        if not isinstance(tr.data, np.ma.MaskedArray):\n",
    "                             tr.data = np.ma.masked_array(tr.data, mask=invalid_mask)\n",
    "                        else:\n",
    "                             # Combine with existing mask using logical OR\n",
    "                             tr.data.mask = np.logical_or(tr.data.mask, invalid_mask)\n",
    "                        # Optional: print message\n",
    "                        # num_masked = np.sum(invalid_mask)\n",
    "                        # print(f\"Masked {num_masked} points with invalid value {invalid_value} in trace {tr.id}.\")\n",
    "\n",
    "                # else: # Optional: handle cases where median couldn't be calculated\n",
    "                #     print(f\"Warning: Could not calculate a valid median for trace {tr.id}. Skipping invalid value masking.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Keep the original trace data if masking fails\n",
    "                print(f\"Warning: Error during invalid value masking for trace {tr.id}: {e}. Proceeding with original data.\")\n",
    "    \n",
    "\n",
    "    # Process gaps with linear interpolation\n",
    "    for tr in st:\n",
    "        linear_interpolation(tr, interpolation_limit=1)\n",
    "\n",
    "    # Remove instrument response if needed\n",
    "    for tr in st:\n",
    "        # On Apollo missions, channels can be MH1, MH2, MHZ, or SHZ, etc.\n",
    "        if tr.stats.channel in ['MH1', 'MH2', 'MHZ']:\n",
    "            original_mask = linear_interpolation(tr, interpolation_limit=None)\n",
    "            if units.upper() == 'DISP':\n",
    "                # Pre-filter band might be different for MH* channels\n",
    "                pre_filt = [1/350, 1/300, 1.5, 2]\n",
    "                tr.remove_response(inventory=inv, pre_filt=pre_filt,\n",
    "                                   output=\"DISP\", water_level=None, plot=False)\n",
    "            tr.data = np.ma.masked_array(tr.data, mask=original_mask)\n",
    "        elif tr.stats.channel in ['SHZ']:\n",
    "            original_mask = linear_interpolation(tr, interpolation_limit=None)\n",
    "            if units.upper() == 'DISP':\n",
    "                # Different pre-filter band for SHZ\n",
    "                pre_filt = [1/100, 1/50, 20, 22]\n",
    "                tr.remove_response(inventory=inv, pre_filt=pre_filt,\n",
    "                                   output=\"DISP\", water_level=None, plot=False)\n",
    "            tr.data = np.ma.masked_array(tr.data, mask=original_mask)\n",
    "\n",
    "    # For simplicity, return just the first Trace in the Stream\n",
    "    return st[0], [starttime, endtime, station, channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a83f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns:\n",
      "['No', 'Year', 'Day of the year', 'Month', 'Day', 'Start Time\\n(HHMM)', 'Stop Time\\n(HHMM)', 'Start Time\\n(HHMM).1', 'Stop Time\\n(HHMM).1', 'Event Date', '30 minutes\\nBefore event', '3 Hours\\nAfter Event', '30 minutes\\nBefore event.1', '3 Hours\\nAfter Event.1', 'Signal envelope amplitudes', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Availability', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Data quality code', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Comment', 'Event Type', 'Unnamed: 28', 'New DMQ', 'Unnamed: 30', 'Article Usage', 'Unnamed: 32']\n",
      "Found 1744 impacts.\n",
      "Availability mapping (station: column index):\n",
      "{'S15': 20, 'S16': 21}\n",
      "Event 2.0 not available at station S15.\n",
      "Event 2.0 not available at station S16.\n",
      "Event 4.0 not available at station S15.\n",
      "Event 4.0 not available at station S16.\n",
      "Event 5.0 not available at station S15.\n",
      "Event 5.0 not available at station S16.\n",
      "Event 6.0 not available at station S15.\n",
      "Event 6.0 not available at station S16.\n",
      "Event 14.0 not available at station S15.\n",
      "Event 14.0 not available at station S16.\n",
      "Event 22.0 not available at station S15.\n",
      "Event 22.0 not available at station S16.\n",
      "Event 27.0 not available at station S15.\n",
      "Event 27.0 not available at station S16.\n",
      "Event 30.0 not available at station S15.\n",
      "Event 30.0 not available at station S16.\n",
      "Event 51.0 not available at station S15.\n",
      "Event 51.0 not available at station S16.\n",
      "Event 55.0 not available at station S15.\n",
      "Event 55.0 not available at station S16.\n",
      "Event 59.0 not available at station S15.\n",
      "Event 59.0 not available at station S16.\n",
      "Event 61.0 not available at station S15.\n",
      "Event 61.0 not available at station S16.\n",
      "Event 64.0 not available at station S15.\n",
      "Event 64.0 not available at station S16.\n",
      "Event 73.0 not available at station S15.\n",
      "Event 73.0 not available at station S16.\n",
      "Event 78.0 not available at station S15.\n",
      "Event 78.0 not available at station S16.\n",
      "Event 104.0 not available at station S15.\n",
      "Event 104.0 not available at station S16.\n",
      "Event 113.0 not available at station S15.\n",
      "Event 113.0 not available at station S16.\n",
      "Event 114.0 not available at station S15.\n",
      "Event 114.0 not available at station S16.\n",
      "Event 146.0 not available at station S15.\n",
      "Event 146.0 not available at station S16.\n",
      "Event 149.0 not available at station S15.\n",
      "Event 149.0 not available at station S16.\n",
      "Event 165.0 not available at station S15.\n",
      "Event 165.0 not available at station S16.\n",
      "Event 169.0 not available at station S15.\n",
      "Event 169.0 not available at station S16.\n",
      "Event 181.0 not available at station S15.\n",
      "Event 181.0 not available at station S16.\n",
      "Event 186.0 not available at station S15.\n",
      "Event 186.0 not available at station S16.\n",
      "Event 197.0 not available at station S15.\n",
      "Event 197.0 not available at station S16.\n",
      "Event 204.0 not available at station S15.\n",
      "Event 204.0 not available at station S16.\n",
      "Event 209.0 not available at station S15.\n",
      "Event 209.0 not available at station S16.\n",
      "Event 215.0 not available at station S15.\n",
      "Event 215.0 not available at station S16.\n",
      "Event 217.0 not available at station S15.\n",
      "Event 217.0 not available at station S16.\n",
      "Event 218.0 not available at station S15.\n",
      "Event 218.0 not available at station S16.\n",
      "Event 240.0 not available at station S15.\n",
      "Event 240.0 not available at station S16.\n",
      "Event 243.0 not available at station S15.\n",
      "Event 243.0 not available at station S16.\n",
      "Event 244.0 not available at station S15.\n",
      "Event 244.0 not available at station S16.\n",
      "Event 267.0 not available at station S15.\n",
      "Event 267.0 not available at station S16.\n",
      "Event 269.0 not available at station S15.\n",
      "Event 269.0 not available at station S16.\n",
      "Event 270.0 not available at station S15.\n",
      "Event 270.0 not available at station S16.\n",
      "Event 271.0 not available at station S15.\n",
      "Event 271.0 not available at station S16.\n",
      "Event 274.0 not available at station S15.\n",
      "Event 274.0 not available at station S16.\n",
      "Event 321.0 not available at station S15.\n",
      "Event 321.0 not available at station S16.\n",
      "Event 337.0 not available at station S15.\n",
      "Event 337.0 not available at station S16.\n",
      "Event 338.0 not available at station S15.\n",
      "Event 338.0 not available at station S16.\n",
      "Event 342.0 not available at station S15.\n",
      "Event 342.0 not available at station S16.\n",
      "Event 354.0 not available at station S15.\n",
      "Event 354.0 not available at station S16.\n",
      "Event 357.0 not available at station S15.\n",
      "Event 357.0 not available at station S16.\n",
      "Event 361.0 not available at station S15.\n",
      "Event 361.0 not available at station S16.\n",
      "Event 362.0 not available at station S15.\n",
      "Event 362.0 not available at station S16.\n",
      "Event 381.0 not available at station S15.\n",
      "Event 381.0 not available at station S16.\n",
      "Event 388.0 not available at station S15.\n",
      "Event 388.0 not available at station S16.\n",
      "Event 393.0 not available at station S15.\n",
      "Event 393.0 not available at station S16.\n",
      "Event 394.0 not available at station S15.\n",
      "Event 394.0 not available at station S16.\n",
      "Error parsing event time for event 418.0: unconverted data remains: 900\n",
      "Event 425.0 not available at station S15.\n",
      "Event 425.0 not available at station S16.\n",
      "Event 445.0 not available at station S15.\n",
      "Event 445.0 not available at station S16.\n",
      "Event 449.0 not available at station S15.\n",
      "Event 449.0 not available at station S16.\n",
      "Event 456.0 not available at station S15.\n",
      "Event 456.0 not available at station S16.\n",
      "Event 467.0 not available at station S15.\n",
      "Event 467.0 not available at station S16.\n",
      "Event 479.0 not available at station S15.\n",
      "Event 479.0 not available at station S16.\n",
      "Event 515.0 not available at station S15.\n",
      "Event 515.0 not available at station S16.\n",
      "Event 536.0 not available at station S15.\n",
      "Event 536.0 not available at station S16.\n",
      "Event 551.0 not available at station S15.\n",
      "Event 551.0 not available at station S16.\n",
      "Event 556.0 not available at station S15.\n",
      "Event 556.0 not available at station S16.\n",
      "Event 582.0 not available at station S15.\n",
      "Event 582.0 not available at station S16.\n",
      "Event 583.0 not available at station S15.\n",
      "Event 583.0 not available at station S16.\n",
      "Event 600.0 not available at station S15.\n",
      "Event 600.0 not available at station S16.\n",
      "Event 609.0 not available at station S15.\n",
      "Event 609.0 not available at station S16.\n",
      "Event 623.0 not available at station S15.\n",
      "Event 623.0 not available at station S16.\n",
      "Event 628.0 not available at station S15.\n",
      "Event 628.0 not available at station S16.\n",
      "Event 631.0 not available at station S15.\n",
      "Event 631.0 not available at station S16.\n",
      "Event 633.0 not available at station S15.\n",
      "Event 633.0 not available at station S16.\n",
      "Error parsing event time for event 652.0: unconverted data remains: 900\n",
      "Event 665.0 not available at station S15.\n",
      "Event 665.0 not available at station S16.\n",
      "Event 668.0 not available at station S15.\n",
      "Event 668.0 not available at station S16.\n",
      "Event 676.0 not available at station S15.\n",
      "Event 676.0 not available at station S16.\n",
      "Event 685.0 not available at station S15.\n",
      "Event 685.0 not available at station S16.\n",
      "Event 696.0 not available at station S15.\n",
      "Event 696.0 not available at station S16.\n",
      "Event 701.0 not available at station S15.\n",
      "Event 701.0 not available at station S16.\n",
      "Event 702.0 not available at station S15.\n",
      "Event 702.0 not available at station S16.\n",
      "Event 717.0 not available at station S15.\n",
      "Event 717.0 not available at station S16.\n",
      "Event 720.0 not available at station S15.\n",
      "Event 720.0 not available at station S16.\n",
      "Event 721.0 not available at station S15.\n",
      "Event 721.0 not available at station S16.\n",
      "Event 723.0 not available at station S15.\n",
      "Event 723.0 not available at station S16.\n",
      "Event 746.0 not available at station S15.\n",
      "Event 746.0 not available at station S16.\n",
      "Event 748.0 not available at station S15.\n",
      "Event 748.0 not available at station S16.\n",
      "Event 751.0 not available at station S15.\n",
      "Event 751.0 not available at station S16.\n",
      "Event 752.0 not available at station S15.\n",
      "Event 752.0 not available at station S16.\n",
      "Event 766.0 not available at station S15.\n",
      "Event 766.0 not available at station S16.\n",
      "Event 768.0 not available at station S15.\n",
      "Event 768.0 not available at station S16.\n",
      "Event 771.0 not available at station S15.\n",
      "Event 771.0 not available at station S16.\n",
      "Event 772.0 not available at station S15.\n",
      "Event 772.0 not available at station S16.\n",
      "Event 773.0 not available at station S15.\n",
      "Event 773.0 not available at station S16.\n",
      "Event 774.0 not available at station S15.\n",
      "Event 774.0 not available at station S16.\n",
      "Event 781.0 not available at station S15.\n",
      "Event 781.0 not available at station S16.\n",
      "Event 782.0 not available at station S15.\n",
      "Event 782.0 not available at station S16.\n",
      "Event 783.0 not available at station S15.\n",
      "Event 783.0 not available at station S16.\n",
      "Event 784.0 not available at station S15.\n",
      "Event 784.0 not available at station S16.\n",
      "Event 788.0 not available at station S15.\n",
      "Event 788.0 not available at station S16.\n",
      "Event 799.0 not available at station S15.\n",
      "Event 799.0 not available at station S16.\n",
      "Event 801.0 not available at station S15.\n",
      "Event 801.0 not available at station S16.\n",
      "Event 838.0 not available at station S15.\n",
      "Event 838.0 not available at station S16.\n",
      "Event 851.0 not available at station S15.\n",
      "Event 851.0 not available at station S16.\n",
      "Event 861.0 not available at station S15.\n",
      "Event 861.0 not available at station S16.\n",
      "Error parsing event time for event 886.0: unconverted data remains: 900\n",
      "Event 888.0 not available at station S15.\n",
      "Event 888.0 not available at station S16.\n",
      "Event 889.0 not available at station S15.\n",
      "Event 889.0 not available at station S16.\n",
      "Event 890.0 not available at station S15.\n",
      "Event 890.0 not available at station S16.\n",
      "Event 891.0 not available at station S15.\n",
      "Event 891.0 not available at station S16.\n",
      "Event 892.0 not available at station S15.\n",
      "Event 892.0 not available at station S16.\n",
      "Event 898.0 not available at station S15.\n",
      "Event 898.0 not available at station S16.\n",
      "Event 899.0 not available at station S15.\n",
      "Event 899.0 not available at station S16.\n",
      "Event 901.0 not available at station S15.\n",
      "Event 901.0 not available at station S16.\n",
      "Event 916.0 not available at station S15.\n",
      "Event 916.0 not available at station S16.\n",
      "Event 922.0 not available at station S15.\n",
      "Event 922.0 not available at station S16.\n",
      "Event 924.0 not available at station S15.\n",
      "Event 924.0 not available at station S16.\n",
      "Event 930.0 not available at station S15.\n",
      "Event 930.0 not available at station S16.\n",
      "Event 934.0 not available at station S15.\n",
      "Event 934.0 not available at station S16.\n",
      "Event 975.0 not available at station S15.\n",
      "Event 975.0 not available at station S16.\n",
      "Event 977.0 not available at station S15.\n",
      "Event 977.0 not available at station S16.\n",
      "Event 986.0 not available at station S15.\n",
      "Event 986.0 not available at station S16.\n",
      "Event 990.0 not available at station S15.\n",
      "Event 990.0 not available at station S16.\n",
      "Event 991.0 not available at station S15.\n",
      "Event 991.0 not available at station S16.\n",
      "Event 997.0 not available at station S15.\n",
      "Event 997.0 not available at station S16.\n",
      "Event 1002.0 not available at station S15.\n",
      "Event 1002.0 not available at station S16.\n",
      "Event 1013.0 not available at station S15.\n",
      "Event 1013.0 not available at station S16.\n",
      "Event 1034.0 not available at station S15.\n",
      "Event 1034.0 not available at station S16.\n",
      "Event 1053.0 not available at station S15.\n",
      "Event 1053.0 not available at station S16.\n",
      "Event 1060.0 not available at station S15.\n",
      "Event 1060.0 not available at station S16.\n",
      "Event 1061.0 not available at station S15.\n",
      "Event 1061.0 not available at station S16.\n",
      "Event 1068.0 not available at station S15.\n",
      "Event 1068.0 not available at station S16.\n",
      "Event 1069.0 not available at station S15.\n",
      "Event 1069.0 not available at station S16.\n",
      "Event 1086.0 not available at station S15.\n",
      "Event 1086.0 not available at station S16.\n",
      "Event 1136.0 not available at station S15.\n",
      "Event 1136.0 not available at station S16.\n",
      "Event 1138.0 not available at station S15.\n",
      "Event 1138.0 not available at station S16.\n",
      "Event 1139.0 not available at station S15.\n",
      "Event 1139.0 not available at station S16.\n",
      "Event 1140.0 not available at station S15.\n",
      "Event 1140.0 not available at station S16.\n",
      "Event 1146.0 not available at station S15.\n",
      "Event 1146.0 not available at station S16.\n",
      "Event 1173.0 not available at station S15.\n",
      "Event 1173.0 not available at station S16.\n",
      "Event 1202.0 not available at station S15.\n",
      "Event 1202.0 not available at station S16.\n",
      "Event 1203.0 not available at station S15.\n",
      "Event 1203.0 not available at station S16.\n",
      "Event 1222.0 not available at station S15.\n",
      "Event 1222.0 not available at station S16.\n",
      "Event 1223.0 not available at station S15.\n",
      "Event 1223.0 not available at station S16.\n",
      "Error parsing event time for event 1233.0: unconverted data remains: 900\n",
      "Event 1238.0 not available at station S15.\n",
      "Event 1238.0 not available at station S16.\n",
      "Event 1239.0 not available at station S15.\n",
      "Event 1239.0 not available at station S16.\n",
      "Event 1248.0 not available at station S15.\n",
      "Event 1248.0 not available at station S16.\n",
      "Event 1266.0 not available at station S15.\n",
      "Event 1266.0 not available at station S16.\n",
      "Event 1267.0 not available at station S15.\n",
      "Event 1267.0 not available at station S16.\n",
      "Event 1268.0 not available at station S15.\n",
      "Event 1268.0 not available at station S16.\n",
      "Error parsing event time for event 1269.0: unconverted data remains: 900\n",
      "Event 1270.0 not available at station S15.\n",
      "Event 1270.0 not available at station S16.\n",
      "Event 1287.0 not available at station S15.\n",
      "Event 1287.0 not available at station S16.\n",
      "Event 1342.0 not available at station S15.\n",
      "Event 1342.0 not available at station S16.\n",
      "Event 1345.0 not available at station S15.\n",
      "Event 1345.0 not available at station S16.\n",
      "Event 1346.0 not available at station S15.\n",
      "Event 1346.0 not available at station S16.\n",
      "Event 1367.0 not available at station S15.\n",
      "Event 1367.0 not available at station S16.\n",
      "Event 1369.0 not available at station S15.\n",
      "Event 1369.0 not available at station S16.\n",
      "Event 1387.0 not available at station S15.\n",
      "Event 1387.0 not available at station S16.\n",
      "Event 1388.0 not available at station S15.\n",
      "Event 1388.0 not available at station S16.\n",
      "Event 1395.0 not available at station S15.\n",
      "Event 1395.0 not available at station S16.\n",
      "Error parsing event time for event 1403.0: unconverted data remains: 900\n",
      "Event 1410.0 not available at station S15.\n",
      "Event 1410.0 not available at station S16.\n",
      "Event 1417.0 not available at station S15.\n",
      "Event 1417.0 not available at station S16.\n",
      "Event 1426.0 not available at station S15.\n",
      "Event 1426.0 not available at station S16.\n",
      "Event 1428.0 not available at station S15.\n",
      "Event 1428.0 not available at station S16.\n",
      "Event 1443.0 not available at station S15.\n",
      "Event 1443.0 not available at station S16.\n",
      "Event 1453.0 not available at station S15.\n",
      "Event 1453.0 not available at station S16.\n",
      "Event 1455.0 not available at station S15.\n",
      "Event 1455.0 not available at station S16.\n",
      "Event 1460.0 not available at station S15.\n",
      "Event 1460.0 not available at station S16.\n",
      "Event 1469.0 not available at station S15.\n",
      "Event 1469.0 not available at station S16.\n",
      "Event 1471.0 not available at station S15.\n",
      "Event 1471.0 not available at station S16.\n",
      "Event 1476.0 not available at station S15.\n",
      "Event 1476.0 not available at station S16.\n",
      "Event 1484.0 not available at station S15.\n",
      "Event 1484.0 not available at station S16.\n",
      "Event 1488.0 not available at station S15.\n",
      "Event 1488.0 not available at station S16.\n",
      "Event 1496.0 not available at station S15.\n",
      "Event 1496.0 not available at station S16.\n",
      "Event 1498.0 not available at station S15.\n",
      "Event 1498.0 not available at station S16.\n",
      "Event 1501.0 not available at station S15.\n",
      "Event 1501.0 not available at station S16.\n",
      "Event 1552.0 not available at station S15.\n",
      "Event 1552.0 not available at station S16.\n",
      "Event 1553.0 not available at station S15.\n",
      "Event 1553.0 not available at station S16.\n",
      "Event 1554.0 not available at station S15.\n",
      "Event 1554.0 not available at station S16.\n",
      "Event 1559.0 not available at station S15.\n",
      "Event 1559.0 not available at station S16.\n",
      "Event 1560.0 not available at station S15.\n",
      "Event 1560.0 not available at station S16.\n",
      "Event 1592.0 not available at station S15.\n",
      "Event 1592.0 not available at station S16.\n",
      "Event 1597.0 not available at station S15.\n",
      "Event 1597.0 not available at station S16.\n",
      "Error parsing event time for event 1599.0: unconverted data remains: 900\n",
      "Event 1600.0 not available at station S15.\n",
      "Event 1600.0 not available at station S16.\n",
      "Event 1601.0 not available at station S15.\n",
      "Event 1601.0 not available at station S16.\n",
      "Event 1603.0 not available at station S15.\n",
      "Event 1603.0 not available at station S16.\n",
      "Error parsing event time for event 1604.0: unconverted data remains: 900\n",
      "Event 1606.0 not available at station S15.\n",
      "Event 1606.0 not available at station S16.\n",
      "Event 1615.0 not available at station S15.\n",
      "Event 1615.0 not available at station S16.\n",
      "Event 1618.0 not available at station S15.\n",
      "Event 1618.0 not available at station S16.\n",
      "Event 1619.0 not available at station S15.\n",
      "Event 1619.0 not available at station S16.\n",
      "Event 1631.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1631.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1631.0, Station S15, Channel SHZ (SP): Saved seismogram.\n",
      "Event 1631.0 not available at station S16.\n",
      "Event 1634.0 not available at station S15.\n",
      "Event 1634.0 not available at station S16.\n",
      "Event 1636.0 not available at station S15.\n",
      "Event 1636.0 not available at station S16.\n",
      "Event 1644.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1644.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1644.0, Station S15, Channel SHZ (SP): Saved seismogram.\n",
      "Event 1644.0 not available at station S16.\n",
      "Event 1647.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1647.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1647.0, Station S15, Channel SHZ (SP): Saved seismogram.\n",
      "Event 1647.0 not available at station S16.\n",
      "Event 1650.0 not available at station S15.\n",
      "Event 1650.0 not available at station S16.\n",
      "Event 1658.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1658.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1658.0, Station S15, Channel SHZ (SP): Saved seismogram.\n",
      "Event 1658.0 not available at station S16.\n",
      "Event 1692.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1692.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1692.0, Station S15, Channel SHZ (SP): Saved seismogram.\n",
      "Event 1692.0 not available at station S16.\n",
      "Event 1693.0 not available at station S15.\n",
      "Event 1693.0 not available at station S16.\n",
      "Event 1699.0 not available at station S15.\n",
      "Event 1699.0 not available at station S16.\n",
      "Event 1762.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1762.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1762.0, Station S15, Channel SHZ (SP): Saved seismogram.\n",
      "Event 1762.0 not available at station S16.\n",
      "Event 1767.0 available at station S15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1767.0, Station S15, Channel MHZ (LP): Saved seismogram.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 212\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubindent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 212\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[12], line 197\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping event \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (already processed).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m process_event(event_row, availability_map)\n\u001b[1;32m    198\u001b[0m processed_events\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mstr\u001b[39m(event_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    199\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[12], line 82\u001b[0m, in \u001b[0;36mprocess_event\u001b[0;34m(event_row, avail_map)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chan \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m         event_trace, _ \u001b[38;5;241m=\u001b[39m load_Apollo(starttime\u001b[38;5;241m=\u001b[39mextended_start, endtime\u001b[38;5;241m=\u001b[39mextended_end,\n\u001b[1;32m     83\u001b[0m                                      station\u001b[38;5;241m=\u001b[39mstation, channel\u001b[38;5;241m=\u001b[39mchan, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event_trace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m event_trace\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(event_trace\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchan\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] No event data – skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mload_Apollo\u001b[0;34m(starttime, endtime, station, channel, units, year, month)\u001b[0m\n\u001b[1;32m     50\u001b[0m inv \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_stations(starttime\u001b[38;5;241m=\u001b[39mstarttime, endtime\u001b[38;5;241m=\u001b[39mendtime,\n\u001b[1;32m     51\u001b[0m                           network\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA\u001b[39m\u001b[38;5;124m'\u001b[39m, sta\u001b[38;5;241m=\u001b[39mstation, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, channel\u001b[38;5;241m=\u001b[39mchannel,\n\u001b[1;32m     52\u001b[0m                           level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Retrieve waveforms\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m st \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_waveforms(network\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA\u001b[39m\u001b[38;5;124m'\u001b[39m, station\u001b[38;5;241m=\u001b[39mstation, channel\u001b[38;5;241m=\u001b[39mchannel, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m                           starttime\u001b[38;5;241m=\u001b[39mstarttime, endtime\u001b[38;5;241m=\u001b[39mendtime)\n\u001b[1;32m     57\u001b[0m st\u001b[38;5;241m.\u001b[39mmerge()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m st:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/site-packages/obspy/clients/fdsn/client.py:872\u001b[0m, in \u001b[0;36mClient.get_waveforms\u001b[0;34m(self, network, station, location, channel, starttime, endtime, quality, minimumlength, longestonly, filename, attach_response, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_url_from_parameters(\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataselect\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_PARAMETERS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataselect\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs)\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# Gzip not worth it for MiniSEED and most likely disabled for this\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# route in any case.\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download(url, use_gzip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    873\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/site-packages/obspy/clients/fdsn/client.py:1482\u001b[0m, in \u001b[0;36mClient._download\u001b[0;34m(self, url, return_string, data, use_gzip, content_type)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type:\n\u001b[1;32m   1481\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content_type\n\u001b[0;32m-> 1482\u001b[0m code, data \u001b[38;5;241m=\u001b[39m download_url(\n\u001b[1;32m   1483\u001b[0m     url, opener\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_opener, headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1484\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug, return_string\u001b[38;5;241m=\u001b[39mreturn_string, data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1485\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, use_gzip\u001b[38;5;241m=\u001b[39muse_gzip)\n\u001b[1;32m   1486\u001b[0m raise_on_error(code, data)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/site-packages/obspy/clients/fdsn/client.py:1970\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, opener, timeout, headers, debug, return_string, data, use_gzip)\u001b[0m\n\u001b[1;32m   1967\u001b[0m     f \u001b[38;5;241m=\u001b[39m url_obj\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_string \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 1970\u001b[0m     data \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1972\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_chunked(amt)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:595\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    593\u001b[0m value \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (chunk_left \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_chunk_left()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_left:\n\u001b[1;32m    597\u001b[0m             value\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(amt))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:579\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_next_chunk_size()\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:539\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from obspy import UTCDateTime, read\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# -----------------------------\n",
    "# Helper function: preprocess_trace\n",
    "def preprocess_trace(trace):\n",
    "    \"\"\"\n",
    "    Pre-processing workflow:\n",
    "      1. Fill masked data with interpolation if possible\n",
    "      2. Detrend (linear) and demean\n",
    "      3. Apply de-glitching\n",
    "    \"\"\"\n",
    "    proc = trace.copy()\n",
    "    \n",
    "    # Handle masked data\n",
    "    if np.ma.is_masked(proc.data):\n",
    "        mask = np.ma.getmaskarray(proc.data)\n",
    "        if np.all(mask):\n",
    "            proc.data = np.zeros_like(proc.data, dtype=float)\n",
    "        else:\n",
    "            x = np.arange(len(proc.data))\n",
    "            valid_indices = ~mask\n",
    "            if np.any(valid_indices):\n",
    "                valid_x = x[valid_indices]\n",
    "                valid_y = proc.data[valid_indices]\n",
    "                proc.data = np.interp(x, valid_x, valid_y)\n",
    "            else:\n",
    "                proc.data = proc.data.filled(0)\n",
    "    \n",
    "    # Detrend and demean\n",
    "    proc.detrend(type='linear')\n",
    "    proc.detrend(type='demean')\n",
    "    \n",
    "    # Apply de-glitching\n",
    "    #proc.data = deglitch_data(proc.data, threshold=3)\n",
    "    \n",
    "    return proc\n",
    "\n",
    "# -----------------------------\n",
    "# Function: Process Event\n",
    "def process_event(event_row, avail_map):\n",
    "    # Parse event time\n",
    "    try:\n",
    "        event_date_str = event_row[\"Event Date\"]\n",
    "        year_str, month_str, day_time = event_date_str.split('/', 2)\n",
    "        day_str, time_str = day_time.split(' ', 1)\n",
    "        full_year = 1900 + int(year_str)\n",
    "        starttime = UTCDateTime(f\"{full_year}-{month_str.zfill(2)}-{day_str.zfill(2)}T{time_str}:00\")\n",
    "        stop_time_str = str(int(event_row[\"Stop Time\\n(HHMM)\"])).zfill(4)\n",
    "        stop_hour = stop_time_str[:2]\n",
    "        stop_min = stop_time_str[2:]\n",
    "        endtime = UTCDateTime(f\"{full_year}-{month_str.zfill(2)}-{day_str.zfill(2)}T{stop_hour}:{stop_min}:00\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing event time for event {event_row['No']}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Notice: change event duration here if needed\n",
    "    event_time_str = starttime.strftime(\"%Y%m%dT%H%M%S\")\n",
    "    extended_start = starttime - 900  # 15 minutes before start\n",
    "    extended_end = endtime + 1800    # 30 minutes after stop\n",
    "\n",
    "    # Sensor-channel definitions\n",
    "    sensors_channels = {\n",
    "        \"LP\": [\"MHZ\"], # Notice: change here if you need other components, for example, \"LP\": [\"MH1\", \"MH2\", \"MHZ\"],\n",
    "        \"SP\": [\"SHZ\"]\n",
    "    }\n",
    "\n",
    "    for station, col_index in avail_map.items():\n",
    "        if pd.notna(event_row.iloc[col_index]):\n",
    "            print(f\"Event {event_row['No']} available at station {station}.\")\n",
    "            \n",
    "            # Process LP and SP\n",
    "            for sensor, channels in sensors_channels.items():\n",
    "                for chan in channels:\n",
    "                    try:\n",
    "                        event_trace, _ = load_Apollo(starttime=extended_start, endtime=extended_end,\n",
    "                                                     station=station, channel=chan, units=\"DU\")\n",
    "                        if event_trace is None or event_trace.data is None or len(event_trace.data) == 0:\n",
    "                            print(f\"[{station} {chan} {sensor}] No event data – skipping.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Preprocess the trace\n",
    "                        event_trace = preprocess_trace(event_trace)\n",
    "                        \n",
    "                        # Create output folder\n",
    "                        output_folder = os.path.join(\"Impact_test\", station, sensor, event_time_str, chan)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        \n",
    "                        # Save seismogram text\n",
    "                        seismogram_txt = os.path.join(output_folder, \"seismogram.txt\")\n",
    "                        time_datetimes = [(event_trace.stats.starttime + i * event_trace.stats.delta).datetime \n",
    "                                         for i in range(len(event_trace.data))]\n",
    "                        time_strings = [t.isoformat() for t in time_datetimes]\n",
    "                        with open(seismogram_txt, \"w\") as f:\n",
    "                            f.write(\"Time Amplitude\\n\")\n",
    "                            for ts, amplitude in zip(time_strings, event_trace.data):\n",
    "                                f.write(f\"{ts} {amplitude:f}\\n\")\n",
    "                        \n",
    "                        # Save seismogram plot\n",
    "                        plt.figure()\n",
    "                        plt.plot(time_datetimes, event_trace.data)\n",
    "                        plt.xlabel(\"Time\")\n",
    "                        plt.ylabel(\"Amplitude\")\n",
    "                        plt.title(f\"Seismogram for {station} {chan} ({sensor})\")\n",
    "                        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "                        plt.gcf().autofmt_xdate()\n",
    "                        seismogram_png = os.path.join(output_folder, \"seismogram.png\")\n",
    "                        plt.savefig(seismogram_png)\n",
    "                        plt.close()\n",
    "                        \n",
    "                        print(f\"Event {event_row['No']}, Station {station}, Channel {chan} ({sensor}): Saved seismogram.\")\n",
    "                        \n",
    "                        # Clean up\n",
    "                        del event_trace\n",
    "                        gc.collect()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing station {station} channel {chan} ({sensor}): {e}\")\n",
    "                        continue\n",
    "        else:\n",
    "            print(f\"Event {event_row['No']} not available at station {station}.\")\n",
    "\n",
    "    with open(\"checkpoint_impact_test.txt\", \"a\") as f:\n",
    "        f.write(f\"Processed event {event_row['No']} at {UTCDateTime.now()}\\n\")\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Main Function\n",
    "def main():\n",
    "    csv_file = \"Nakamura_MQCatalog.levent.1008 16.21.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"CSV Columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Filter \n",
    "    cols = df.columns.tolist()\n",
    "    # For Shallow moonquakes and Impacts, use the following\n",
    "    try:\n",
    "        idx_et = cols.index(\"Event Type\")\n",
    "    except Exception as e:\n",
    "        print(\"Error: 'Event Type' column not found.\", e)\n",
    "        return\n",
    "    filtered_events = df[(df[\"Event Type\"].str.strip() == \"C\")] #For meteoriod impacts\n",
    "    #filtered_events = df[(df[\"Event Type\"].str.strip() == \"H\")] #For shallow moonquakes\n",
    "    print(f\"Found {len(filtered_events)} impacts.\")\n",
    "\n",
    "    # For Deep moonquakes, use the following\n",
    "\n",
    "    #et_col   = next(c for c in cols if \"New DMQ\" in c)\n",
    "    #idx_et   = cols.index(et_col)\n",
    "    #classifier_col = cols[idx_et+1]\n",
    "    #df[classifier_col] = pd.to_numeric(\n",
    "    #    df[classifier_col].astype(str).str.strip(),\n",
    "    #    errors='coerce'\n",
    "    #)\n",
    "    #filt = df[(df[et_col].str.strip()==\"A\") & (df[classifier_col]==7)]\n",
    "    # Notice: if impact\n",
    "    # filt = df[(df[et_col].str.strip()==\"A\")\n",
    "    #print(f\"{len(filt)} A7 events\")\n",
    "\n",
    "    # Availability mapping, uncomment if you need other stations\n",
    "    availability_map = {\n",
    "        #\"S12\": cols.index(\"Availability\"),\n",
    "        #\"S14\": cols.index(\"Availability\") + 1,\n",
    "        \"S15\": cols.index(\"Availability\") + 2,\n",
    "        \"S16\": cols.index(\"Availability\") + 3\n",
    "    }\n",
    "    print(\"Availability mapping (station: column index):\")\n",
    "    print(availability_map)\n",
    "\n",
    "    # Checkpoint handling\n",
    "    checkpoint_file = \"checkpoint_impact_test.txt\"\n",
    "    processed_events = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"Processed event\"):\n",
    "                    parts = line.split()\n",
    "                    event_no = parts[2]\n",
    "                    processed_events.add(event_no)\n",
    "\n",
    "    # Process events\n",
    "    for idx, event_row in filtered_events.iterrows():\n",
    "        if str(event_row[\"No\"]) in processed_events:\n",
    "            print(f\"Skipping event {event_row['No']} (already processed).\")\n",
    "            continue\n",
    "        process_event(event_row, availability_map)\n",
    "        processed_events.add(str(event_row['No']))\n",
    "        gc.collect()\n",
    "\n",
    "    # Print folder structure\n",
    "    print(\"Final folder structure (relative paths):\")\n",
    "    for root, dirs, files in os.walk(\"Impact_test\"):\n",
    "        level = root.replace(\"Impact_test\", \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = \" \" * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bf6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns:\n",
      "['Event ID', 'Year', 'DOY', 'Start time (UTC)', 'Ts–Tp (s)', 'Hypocentral distance (km)', 'Min. fc (Hz)', 'Min. Erel (J)', 'Min. M0 (Nm)', 'Min. Δσ (MPa)', 'Min. mb', 'Station']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐1, Station S15, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐2, Station S14, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐2, Station S15, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐3, Station S15, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐3, Station S14, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐4, Station S14, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/rybd089s7ssd_ybsnsgsxvd40000gp/T/ipykernel_39066/2006576976.py:27: FutureWarning: The 'downcast' keyword in Series.interpolate is deprecated and will be removed in a future version. Call result.infer_objects(copy=False) on the result instead.\n",
      "  data_series.interpolate(method='linear', axis=0, limit=interpolation_limit, inplace=True, limit_direction=None, limit_area='inside', downcast=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event KO‐SMQ‐4, Station S15, Channel SHZ (SP): Saved seismogram.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 181\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubindent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 181\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[14], line 166\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping event \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (already processed).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m process_event(event_row)\n\u001b[1;32m    167\u001b[0m processed_events\u001b[38;5;241m.\u001b[39madd(event_id)\n\u001b[1;32m    168\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[14], line 88\u001b[0m, in \u001b[0;36mprocess_event\u001b[0;34m(event_row)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m stations:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m         event_trace, _ \u001b[38;5;241m=\u001b[39m load_Apollo(starttime\u001b[38;5;241m=\u001b[39mstarttime, endtime\u001b[38;5;241m=\u001b[39mendtime,\n\u001b[1;32m     89\u001b[0m                                      station\u001b[38;5;241m=\u001b[39mstation, channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event_trace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m event_trace\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(event_trace\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m SHZ SP] No event data – skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mload_Apollo\u001b[0;34m(starttime, endtime, station, channel, units, year, month)\u001b[0m\n\u001b[1;32m     50\u001b[0m inv \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_stations(starttime\u001b[38;5;241m=\u001b[39mstarttime, endtime\u001b[38;5;241m=\u001b[39mendtime,\n\u001b[1;32m     51\u001b[0m                           network\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA\u001b[39m\u001b[38;5;124m'\u001b[39m, sta\u001b[38;5;241m=\u001b[39mstation, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, channel\u001b[38;5;241m=\u001b[39mchannel,\n\u001b[1;32m     52\u001b[0m                           level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Retrieve waveforms\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m st \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_waveforms(network\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA\u001b[39m\u001b[38;5;124m'\u001b[39m, station\u001b[38;5;241m=\u001b[39mstation, channel\u001b[38;5;241m=\u001b[39mchannel, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m                           starttime\u001b[38;5;241m=\u001b[39mstarttime, endtime\u001b[38;5;241m=\u001b[39mendtime)\n\u001b[1;32m     57\u001b[0m st\u001b[38;5;241m.\u001b[39mmerge()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m st:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/site-packages/obspy/clients/fdsn/client.py:872\u001b[0m, in \u001b[0;36mClient.get_waveforms\u001b[0;34m(self, network, station, location, channel, starttime, endtime, quality, minimumlength, longestonly, filename, attach_response, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_url_from_parameters(\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataselect\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_PARAMETERS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataselect\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs)\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# Gzip not worth it for MiniSEED and most likely disabled for this\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# route in any case.\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download(url, use_gzip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    873\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/site-packages/obspy/clients/fdsn/client.py:1482\u001b[0m, in \u001b[0;36mClient._download\u001b[0;34m(self, url, return_string, data, use_gzip, content_type)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type:\n\u001b[1;32m   1481\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content_type\n\u001b[0;32m-> 1482\u001b[0m code, data \u001b[38;5;241m=\u001b[39m download_url(\n\u001b[1;32m   1483\u001b[0m     url, opener\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_opener, headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1484\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug, return_string\u001b[38;5;241m=\u001b[39mreturn_string, data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1485\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, use_gzip\u001b[38;5;241m=\u001b[39muse_gzip)\n\u001b[1;32m   1486\u001b[0m raise_on_error(code, data)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/site-packages/obspy/clients/fdsn/client.py:1911\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, opener, timeout, headers, debug, return_string, data, use_gzip)\u001b[0m\n\u001b[1;32m   1909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_gzip:\n\u001b[1;32m   1910\u001b[0m         request\u001b[38;5;241m.\u001b[39madd_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1911\u001b[0m     url_obj \u001b[38;5;241m=\u001b[39m opener\u001b[38;5;241m.\u001b[39mopen(request, timeout\u001b[38;5;241m=\u001b[39mtimeout, data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# Catch HTTP errors.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib_request\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1914\u001b[0m     \u001b[38;5;66;03m# try hard to assemble the most details on what the problem is from the\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;66;03m# exception object\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    533\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/urllib/request.py:1373\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPConnection, req)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1348\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/obspy/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## shallow moonquakes onodera2024\n",
    "# SP SHZ only\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from obspy import UTCDateTime, read\n",
    "from obspy.clients.fdsn import Client\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Helper function: preprocess_trace (unchanged)\n",
    "def preprocess_trace(trace):\n",
    "    \"\"\"\n",
    "    Pre-processing workflow:\n",
    "      1. Fill masked data with interpolation if possible\n",
    "      2. Detrend (linear) and demean\n",
    "      3. Apply de-glitching\n",
    "    \"\"\"\n",
    "    proc = trace.copy()\n",
    "    \n",
    "    if np.ma.is_masked(proc.data):\n",
    "        mask = np.ma.getmaskarray(proc.data)\n",
    "        if np.all(mask):\n",
    "            proc.data = np.zeros_like(proc.data, dtype=float)\n",
    "        else:\n",
    "            x = np.arange(len(proc.data))\n",
    "            valid_indices = ~mask\n",
    "            if np.any(valid_indices):\n",
    "                valid_x = x[valid_indices]\n",
    "                valid_y = proc.data[valid_indices]\n",
    "                proc.data = np.interp(x, valid_x, valid_y)\n",
    "            else:\n",
    "                proc.data = proc.data.filled(0)\n",
    "    \n",
    "    proc.detrend(type='linear')\n",
    "    proc.detrend(type='demean')\n",
    "    \n",
    "    #proc.data = deglitch_data(proc.data, threshold=3)\n",
    "    \n",
    "    return proc\n",
    "\n",
    "# Helper function: parse_stations\n",
    "def parse_stations(station_str):\n",
    "    \"\"\"\n",
    "    Parse the Station column to extract a list of stations.\n",
    "    Format examples: 'S15', 'S14 (+S15)', 'S14 (+S15, S16)'\n",
    "    \"\"\"\n",
    "    if \"(\" in station_str:\n",
    "        main_station, additional = station_str.split(\" (+\")\n",
    "        main_station = main_station.strip()\n",
    "        additional = additional.replace(\")\", \"\").split(\",\")\n",
    "        additional_stations = [s.strip() for s in additional]\n",
    "        stations = [main_station] + additional_stations\n",
    "    else:\n",
    "        stations = [station_str.strip()]\n",
    "    return stations\n",
    "\n",
    "# Function: Process Event\n",
    "def process_event(event_row):\n",
    "    # Parse event time from Year, DOY, and Start time (UTC)\n",
    "    try:\n",
    "        year = int(event_row[\"Year\"])\n",
    "        doy = int(event_row[\"DOY\"])\n",
    "        start_time_str = event_row[\"Start time (UTC)\"]\n",
    "        # Convert DOY to date\n",
    "        date = datetime(year, 1, 1) + timedelta(days=doy - 1)\n",
    "        # Parse start time (HH:MM:SS)\n",
    "        hour, minute, second = map(int, start_time_str.split(':'))\n",
    "        start_datetime = datetime(date.year, date.month, date.day, hour, minute, second)\n",
    "        starttime = UTCDateTime(start_datetime)\n",
    "        endtime = starttime + 1800  # 30 minutes after start\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing event time for event {event_row['Event ID']}: {e}\")\n",
    "        return\n",
    "\n",
    "    event_time_str = starttime.strftime(\"%Y%m%dT%H%M%S\")\n",
    "    \n",
    "    # Parse stations from the Station column\n",
    "    station_str = event_row[\"Station\"]\n",
    "    stations = parse_stations(station_str)\n",
    "    \n",
    "    # Process only SP SHZ data for each station\n",
    "    for station in stations:\n",
    "        try:\n",
    "            event_trace, _ = load_Apollo(starttime=starttime, endtime=endtime,\n",
    "                                         station=station, channel=\"SHZ\", units=\"DU\")\n",
    "            if event_trace is None or event_trace.data is None or len(event_trace.data) == 0:\n",
    "                print(f\"[{station} SHZ SP] No event data – skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Preprocess the trace\n",
    "            event_trace = preprocess_trace(event_trace)\n",
    "            \n",
    "            # Create output folder\n",
    "            output_folder = os.path.join(\"Shallow_test\", station, \"SP\", event_time_str, \"SHZ\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            \n",
    "            # Save seismogram text\n",
    "            seismogram_txt = os.path.join(output_folder, \"seismogram.txt\")\n",
    "            time_datetimes = [(event_trace.stats.starttime + i * event_trace.stats.delta).datetime \n",
    "                              for i in range(len(event_trace.data))]\n",
    "            time_strings = [t.isoformat() for t in time_datetimes]\n",
    "            with open(seismogram_txt, \"w\") as f:\n",
    "                f.write(\"Time Amplitude\\n\")\n",
    "                for ts, amplitude in zip(time_strings, event_trace.data):\n",
    "                    f.write(f\"{ts} {amplitude:f}\\n\")\n",
    "            \n",
    "            # Save seismogram plot\n",
    "            plt.figure()\n",
    "            plt.plot(time_datetimes, event_trace.data)\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.title(f\"Seismogram for {station} SHZ (SP)\")\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "            plt.gcf().autofmt_xdate()\n",
    "            seismogram_png = os.path.join(output_folder, \"seismogram.png\")\n",
    "            plt.savefig(seismogram_png)\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Event {event_row['Event ID']}, Station {station}, Channel SHZ (SP): Saved seismogram.\")\n",
    "            \n",
    "            # Clean up\n",
    "            del event_trace\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing station {station} channel SHZ (SP): {e}\")\n",
    "            continue\n",
    "\n",
    "    # Update checkpoint\n",
    "    with open(\"checkpoint_shallow_test.txt\", \"a\") as f:\n",
    "        f.write(f\"Processed event {event_row['Event ID']} at {UTCDateTime.now()}\\n\")\n",
    "    gc.collect()\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    csv_file = \"Onodera2024_TableA1_New_Shallow_Moonquakes_Complete.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"CSV Columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Checkpoint handling\n",
    "    checkpoint_file = \"checkpoint_shallow_test.txt\"\n",
    "    processed_events = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"Processed event\"):\n",
    "                    parts = line.split()\n",
    "                    event_id = parts[2]\n",
    "                    processed_events.add(event_id)\n",
    "\n",
    "    # Process all events (no filtering needed as all are shallow moonquakes)\n",
    "    for idx, event_row in df.iterrows():\n",
    "        event_id = event_row[\"Event ID\"]\n",
    "        if event_id in processed_events:\n",
    "            print(f\"Skipping event {event_id} (already processed).\")\n",
    "            continue\n",
    "        process_event(event_row)\n",
    "        processed_events.add(event_id)\n",
    "        gc.collect()\n",
    "\n",
    "    # Print folder structure\n",
    "    print(\"Final folder structure (relative paths):\")\n",
    "    for root, dirs, files in os.walk(\"Shallow_test\"):\n",
    "        level = root.replace(\"Shallow_test\", \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = \" \" * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
